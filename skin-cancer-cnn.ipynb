{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e90836a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "import random\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt \n",
    "from matplotlib.colors import Normalize,LinearSegmentedColormap\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "import sklearn.exceptions\n",
    "from sklearn.utils import class_weight\n",
    "warnings.filterwarnings(\"ignore\", category=sklearn.exceptions.UndefinedMetricWarning)\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "from tensorflow.keras.utils import load_img, img_to_array\n",
    "from tensorflow.keras.models import load_model\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Dense,Input,Dropout,GlobalAveragePooling2D,Flatten,Conv2D,BatchNormalization,Activation,MaxPooling2D, ZeroPadding2D\n",
    "from keras.models import Model,Sequential\n",
    "from keras.optimizers import Adam,SGD,RMSprop\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from keras import applications\n",
    "finish_sound = \"afplay /Users/mehmet/Documents/vs-code/winsquare.mp3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4a8b497",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/device:CPU:0']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x.name for x in device_lib.list_local_devices()]\n",
    "# !conda install -y -n ml ipykernel=6.23.2 numpy==1.24.0 matplotlib=3.7.1 pandas=2.0.2 seaborn=0.12.1 scikit-learn=1.3.2 tensorflow=2.11.1\n",
    "# !jupyter nbconvert --to html skin-cancer-cnn.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e22d19b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['akiec' 'bcc' 'bkl' 'df' 'mel' 'nv' 'vasc'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train and test data paths\n",
    "train_path = \"dataverse_files/HAM10000_images_pca\"\n",
    "test_path = \"dataverse_files/ISIC2018_Task3_Test_Images\"\n",
    "\n",
    "# Read the data\n",
    "df = pd.read_csv('dataverse_files/HAM10000_metadata.csv')\n",
    "df_test = pd.read_csv('dataverse_files/ISIC2018_Task3_Test_GroundTruth.csv')\n",
    "labels = df['dx'].sort_values().unique()\n",
    "\n",
    "# Add .jpg to image_id column\n",
    "df['image_id'] = df['image_id'].astype(str) + '.jpg'\n",
    "df_test['image_id'] = df_test['image_id'].astype(str) + '.jpg'\n",
    "\n",
    "# Drop unused columns\n",
    "df=df.drop(['lesion_id',  'dx_type', 'age', 'sex', 'localization', 'dataset'], axis=1)\n",
    "df_test=df_test.drop(['lesion_id',  'dx_type', 'age', 'sex', 'localization', 'dataset'], axis=1)\n",
    "\n",
    "# 'ISIC_0035068.jpg' is missing in the test set file, lets remove it from test set dataframe\n",
    "df_test = df_test[df_test['image_id'] != 'ISIC_0035068.jpg']\n",
    "\n",
    "print(labels,'\\n')\n",
    "\n",
    "df.sort_values(by=['image_id'], inplace=True)\n",
    "df.reset_index(inplace=True, drop=True)\n",
    "df_test.sort_values(by=['image_id'], inplace=True)\n",
    "df_test.reset_index(inplace=True, drop=True)\n",
    "\n",
    "train_df, val_df=train_test_split(df, train_size=0.9, shuffle=True, random_state=123, stratify=df['dx'])\n",
    "train_df.reset_index(inplace=True, drop=True)\n",
    "val_df.reset_index(inplace=True, drop=True)\n",
    "test_df = df_test.copy().sample(frac=1, random_state=123).reset_index(drop=True) # shuffle test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8758de9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To use augmented data\n",
    "train_path = \"dataverse_files/HAM10000_images_pca_augmented\"\n",
    "train_df = pd.read_csv('dataverse_files/HAM10000_metadata_augmented_train.csv')\n",
    "val_df = pd.read_csv('dataverse_files/HAM10000_metadata_augmented_val.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90e30a41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "akiec  (297, 50, 43)\n",
      "bcc    (479, 50, 93)\n",
      "bkl    (500, 88, 217)\n",
      "df     (200, 50, 44)\n",
      "mel    (500, 50, 171)\n",
      "nv     (500, 100, 908)\n",
      "vasc   (200, 50, 35)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((2676, 3), (438, 3), (1511, 2))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for label in labels:\n",
    "    list1 = len(train_df[train_df['dx'] == label]), len(val_df[val_df['dx'] == label]), len(test_df[test_df['dx'] == label])\n",
    "    space = ' '\n",
    "    print(label,(5-len(label))*space ,list1)\n",
    "    \n",
    "train_df.shape, val_df.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05200234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2676 validated image filenames belonging to 7 classes.\n",
      "Found 438 validated image filenames belonging to 7 classes.\n",
      "Found 1511 validated image filenames belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "rescale=1./255\n",
    "color_mode = 'rgb'\n",
    "target_size = (28, 28)\n",
    "batch_size  = 64\n",
    "# 600 x 450\n",
    "train_data_np = np.array([img_to_array(load_img(train_path+'/'+img, target_size=target_size)) for img in train_df['image_id'].values.tolist()])\n",
    "\n",
    "datagen = ImageDataGenerator(rescale=rescale,\n",
    "                            featurewise_center=True,\n",
    "                            featurewise_std_normalization=True)\n",
    "datagen.fit(train_data_np)\n",
    "train_set = datagen.flow_from_dataframe(train_df,\n",
    "                                        directory=train_path,\n",
    "                                        x_col=\"image_id\",\n",
    "                                        y_col=\"dx\",\n",
    "                                        color_mode=color_mode,\n",
    "                                        target_size=target_size,\n",
    "                                        batch_size=batch_size,\n",
    "                                        class_mode='categorical',\n",
    "                                        shuffle=False\n",
    "                                        )\n",
    "\n",
    "val_set = datagen.flow_from_dataframe(val_df,\n",
    "                                      directory=train_path,\n",
    "                                      x_col=\"image_id\",\n",
    "                                      y_col=\"dx\",\n",
    "                                      color_mode=color_mode,\n",
    "                                      target_size=target_size,\n",
    "                                      batch_size=batch_size,\n",
    "                                      class_mode='categorical',\n",
    "                                      shuffle=False\n",
    "                                      )\n",
    "test_set = datagen.flow_from_dataframe(test_df,\n",
    "                                       directory=test_path,\n",
    "                                       x_col=\"image_id\",\n",
    "                                       y_col=\"dx\",\n",
    "                                       color_mode=color_mode,\n",
    "                                       target_size=target_size,\n",
    "                                       batch_size=batch_size,\n",
    "                                       class_mode='categorical',\n",
    "                                       shuffle=False\n",
    "                                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ce9b074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 26, 26, 32)        896       \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 26, 26, 32)       128       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " activation (Activation)     (None, 26, 26, 32)        0         \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 13, 13, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 13, 13, 32)        0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 9, 9, 64)          51264     \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 9, 9, 64)         256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 9, 9, 64)          0         \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 4, 4, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 4, 4, 64)          0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 1024)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                65600     \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 64)                0         \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " activation_3 (Activation)   (None, 32)                0         \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 32)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 7)                 231       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 120,455\n",
      "Trainable params: 120,263\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "no_of_classes = 7\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "#1st CNN layer\n",
    "model.add(Conv2D(filters=32, \n",
    "                 kernel_size=(3,3), \n",
    "                 input_shape=(target_size[0],target_size[1],3)\n",
    "                 ))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size = (2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "#2nd CNN layer\n",
    "model.add(Conv2D(filters=64, \n",
    "                 kernel_size=(5,5),\n",
    "                ))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size = (2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "\n",
    "# Passing it to a Fully Connected layer\n",
    "model.add(Flatten())\n",
    "\n",
    "# Fully Connected layer\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# Fully Connected layer\n",
    "model.add(Dense(32))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "#Last Layer\n",
    "model.add(Dense(no_of_classes, activation='softmax'))\n",
    "\n",
    "\n",
    "#model.add(GlobalAveragePooling2D()) # for last cnn layer before flatten\n",
    "\n",
    "#model.add(BatchNormalization()) # can be used for all layers except output layer (better for cnn layers)\n",
    "\n",
    "#model.add(Dropout(0.25)) # after activation\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa00a34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_plot(model, history, now, save=True):\n",
    "    # convert the history.history dict to a pandas DataFrame:\n",
    "    if type(history) is not pd.DataFrame:\n",
    "        history = pd.DataFrame(history)\n",
    "    if save == True:\n",
    "        hist_csv_file = f'model-comparison/{now}/history.csv'\n",
    "        with open(hist_csv_file, mode='w') as f:\n",
    "            history.to_csv(f) \n",
    "    epochs = range(1, history.shape[0]+1)\n",
    "    plt.figure(figsize=(5, 2))\n",
    "    plt.plot(epochs, history['accuracy'], label='Accuracy')\n",
    "    plt.plot(epochs, history['val_accuracy'], label='Validation Accuracy')\n",
    "    max_val_acc_epoch = np.argmax(history['val_accuracy']) + 1\n",
    "    max_val_acc = history['val_accuracy'][max_val_acc_epoch-1]\n",
    "    label='Best Epoch = '+str(max_val_acc_epoch)+'\\nVal. Acc. = '+str((max_val_acc*100).round(2))+ '%'\n",
    "    plt.plot(max_val_acc_epoch, max_val_acc, 'ro', label=label)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlim([0, history.shape[0]+0.1])\n",
    "    plt.ylim([0.5, 1])\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.legend(loc='upper left')\n",
    "    if save == True:\n",
    "        plt.savefig(f'model-comparison/{now}/val-acc.png')\n",
    "        np.savetxt('model-comparison/{}/{}.txt'.format(now,str((max_val_acc*100).round(2))), [max_val_acc], fmt='%f')\n",
    "        stats = str(now) + ' ' + str((max_val_acc*100).round(2)) + '\\n'\n",
    "        with open('model-comparison/best-models.txt', 'a') as f:\n",
    "            f.write(stats)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "07883172",
   "metadata": {},
   "outputs": [],
   "source": [
    "def EvaluateModel(model, test_set, str1, now, save = True):\n",
    "    \n",
    "    print('\\n PREDICTING LABELS OF TEST IMAGES')\n",
    "    result = model.predict(test_set)\n",
    "    y_pred = np.argmax(result, axis=1)\n",
    "    \n",
    "    if save==True:\n",
    "        #save y_pred to csv file\n",
    "        os.mkdir('model-comparison/'+now+'/'+str1)\n",
    "        np.savetxt('model-comparison/{}/{}/pred.csv'.format(now,str1), y_pred, delimiter=',', fmt='%d')\n",
    "    \n",
    "    y_true = test_set.classes # List containing true labels for each image.\n",
    "\n",
    "    # Understanding classification power of model on each class    \n",
    "    report = classification_report(y_true, y_pred, target_names=test_set.class_indices.keys())\n",
    "    report_d = pd.DataFrame(classification_report(y_true, y_pred, output_dict=True, target_names=test_set.class_indices.keys())).transpose()\n",
    "    report_d['support']['accuracy'] = report_d['support']['macro avg']\n",
    "\n",
    "    annot = report_d.copy()\n",
    "    annot.iloc[:, 0:3] = (annot.iloc[:, 0:3]*100).applymap('{:.2f}'.format) + ' %'\n",
    "    annot.iloc[7, 1] = ''\n",
    "    annot.iloc[7, 0] = ''\n",
    "    annot['support'] = annot['support'].astype(int)\n",
    "\n",
    "    # how to save report as image\n",
    "    norm = Normalize(-1,1)\n",
    "    cmap = LinearSegmentedColormap.from_list(\"\", [[norm(-1.0), \"white\"],[norm( 1.0), \"white\"]])\n",
    "    plot = sns.heatmap(report_d, annot=annot, cmap=cmap, cbar=False, fmt='')\n",
    "    fig = plot.get_figure()\n",
    "    if save==True:\n",
    "        fig.savefig('model-comparison/{}/{}/report.png'.format(now,str1))\n",
    "    \n",
    "    f1_score = ((report_d['f1-score']['weighted avg']*100000//10)/100)\n",
    "    accuracy = ((report_d['f1-score']['accuracy']*100000//10)/100)\n",
    "    print('\\nAccuracy of model prediction is: {:.2f} %'.format(accuracy))\n",
    "    print('\\nF1-score of model prediction is: {:.2f} %'.format(f1_score))\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, \n",
    "                              display_labels=test_set.class_indices.keys()\n",
    "                              )\n",
    "    disp.plot(cmap='Reds')\n",
    "    disp.ax_.set_title('Confusion Matrix')\n",
    "    plt.show()\n",
    "    if save==True:\n",
    "        disp.figure_.savefig('model-comparison/{}/{}/cm.png'.format(now,str1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fdccdfb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_new_model(model):\n",
    "    # Extra\n",
    "    #class_weights = class_weight.compute_class_weight(class_weight='balanced',classes=np.unique(train_set.classes), y=train_set.classes)\n",
    "    #class_weights_dict=dict(zip(np.unique(train_set.classes),class_weights))\n",
    "    #keras.utils.set_random_seed(42)     \n",
    "    # inside model.fit: class_weight=class_weights_dict,\n",
    "\n",
    "    # Train new model and evaluate\n",
    "    now = datetime.datetime.now().strftime(\"%d-%m-%H-%M\")\n",
    "    os.mkdir('model-comparison/'+now)\n",
    "    def myprint(s):\n",
    "        with open(f'model-comparison/{now}/modelsummary.txt','a') as f:\n",
    "            print(s, file=f)\n",
    "    model.summary(print_fn=myprint)\n",
    "    with open('model-comparison/last.txt', 'w') as f:\n",
    "        f.write(str(now))\n",
    "    return now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce7589a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train new model and evaluate\n",
    "now = train_new_model(model)\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = Adam(learning_rate=0.01*(batch_size/256), beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "#optimizer = SGD(learning_rate=0.01*(batch_size/256), momentum=0.9, nesterov=True)\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_accuracy', patience=3, verbose=1, factor=0.5, min_lr=0.0000001)\n",
    "checkpoint = ModelCheckpoint(f\"model-comparison/{now}/model.h5\", monitor='val_accuracy', save_best_only=True, mode='max')\n",
    "early_stop = EarlyStopping(monitor='val_accuracy', patience=50, mode='max', restore_best_weights=True)\n",
    "\n",
    "history = model.fit(train_set,\n",
    "                    epochs=100,\n",
    "                    validation_data = val_set,\n",
    "                    callbacks=[reduce_lr, checkpoint, early_stop],\n",
    "                    )\n",
    "\n",
    "loss_plot(model, history.history, now)\n",
    "\n",
    "model = load_model(f\"model-comparison/{now}/model.h5\")\n",
    "EvaluateModel(model, val_set, 'val', now)\n",
    "EvaluateModel(model, test_set, 'test', now)\n",
    "\n",
    "os.system(finish_sound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "95555954",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = load_model(f\"model-comparison/{now}/model.h5\")\n",
    "# EvaluateModel(model, val_set, 'val', now)\n",
    "# EvaluateModel(model, test_set, 'test', now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3f5671e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Trained Model and Evaluate\n",
    "\n",
    "# # now = open('model-comparison/last.txt', 'r').read() \n",
    "# now = '22-11-00-37' # best model so far\n",
    "# model = load_model(f\"model-comparison/{now}/model.h5\")\n",
    "# history = pd.read_csv(f'model-comparison/{now}/history.csv')\n",
    "# loss_plot(model, history, now, save=False)\n",
    "# EvaluateModel(model, val_set, 'val', now, save=False)\n",
    "# EvaluateModel(model, test_set, 'test', now, save=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
