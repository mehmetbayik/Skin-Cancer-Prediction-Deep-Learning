{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e90836a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/device:CPU:0']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import datetime\n",
    "import random # random.seed(42)\n",
    "import warnings\n",
    "import numpy as np # np.random.seed(42)\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt \n",
    "from matplotlib.colors import Normalize,LinearSegmentedColormap\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "import sklearn.exceptions\n",
    "from sklearn.utils import class_weight\n",
    "warnings.filterwarnings(\"ignore\", category=sklearn.exceptions.UndefinedMetricWarning)\n",
    "\n",
    "from tensorflow.config.experimental import enable_op_determinism # enable_op_determinism()\n",
    "from tensorflow.random import set_seed # set_seed(42)\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.utils import load_img, img_to_array, set_random_seed # set_random_seed(42)\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, BatchNormalization, Activation, MaxPooling2D\n",
    "from keras.layers import Input, GlobalAveragePooling2D, ZeroPadding2D\n",
    "from keras.models import Model, Sequential\n",
    "from keras.optimizers import Adam, SGD, RMSprop\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from keras import applications\n",
    "finish_sound = \"afplay /Users/mehmet/Documents/vs-code/winsquare.mp3\"\n",
    "# !conda install -y -n ml ipykernel=6.23.2 numpy==1.24.0 matplotlib=3.7.1 pandas=2.0.2 seaborn=0.12.1 scikit-learn=1.3.2 tensorflow=2.11.1\n",
    "# !jupyter nbconvert --to html skin-cancer-cnn.ipynb\n",
    "[x.name for x in device_lib.list_local_devices()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e22d19b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['akiec' 'bcc' 'bkl' 'mel' 'nv'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train and test data paths\n",
    "train_path = \"dataverse_files/HAM10000_images_pca\"\n",
    "test_path = \"dataverse_files/ISIC2018_Task3_Test_Images\"\n",
    "\n",
    "# Read the data\n",
    "df = pd.read_csv('dataverse_files/HAM10000_metadata.csv')\n",
    "df_test = pd.read_csv('dataverse_files/ISIC2018_Task3_Test_GroundTruth.csv')\n",
    "\n",
    "# Delete df and vasc classes\n",
    "df = df[df['dx'] != 'vasc']\n",
    "df = df[df['dx'] != 'df']\n",
    "df_test = df_test[df_test['dx'] != 'vasc']\n",
    "df_test = df_test[df_test['dx'] != 'df']\n",
    "\n",
    "labels = df['dx'].sort_values().unique()\n",
    "\n",
    "# Add .jpg to image_id column\n",
    "df['image_id'] = df['image_id'].astype(str) + '.jpg'\n",
    "df_test['image_id'] = df_test['image_id'].astype(str) + '.jpg'\n",
    "\n",
    "# Drop unused columns\n",
    "# df=df.drop(['lesion_id',  'dx_type', 'age', 'sex', 'localization', 'dataset'], axis=1)\n",
    "# df_test=df_test.drop(['lesion_id',  'dx_type', 'age', 'sex', 'localization', 'dataset'], axis=1)\n",
    "df=df.drop(['lesion_id', 'dataset'], axis=1)\n",
    "df_test=df_test.drop(['lesion_id', 'dataset'], axis=1)\n",
    "\n",
    "# 'ISIC_0035068.jpg' is missing in the test set file, lets remove it from test set dataframe\n",
    "df_test = df_test[df_test['image_id'] != 'ISIC_0035068.jpg']\n",
    "\n",
    "print(labels,'\\n')\n",
    "\n",
    "df.sort_values(by=['image_id'], inplace=True)\n",
    "df.reset_index(inplace=True, drop=True)\n",
    "df_test.sort_values(by=['image_id'], inplace=True)\n",
    "df_test.reset_index(inplace=True, drop=True)\n",
    "\n",
    "train_df, val_df=train_test_split(df, train_size=0.9, shuffle=True, random_state=123, stratify=df['dx'])\n",
    "train_df.reset_index(inplace=True, drop=True)\n",
    "val_df.reset_index(inplace=True, drop=True)\n",
    "test_df = df_test.copy().sample(frac=1, random_state=123).reset_index(drop=True) # shuffle test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8758de9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To use augmented data\n",
    "x = '_3'\n",
    "train_path = 'dataverse_files/HAM10000_images_pca_augmented'+x\n",
    "train_df = pd.read_csv('dataverse_files/HAM10000_metadata_augmented'+x+'_train.csv')\n",
    "val_df = pd.read_csv('dataverse_files/HAM10000_metadata_augmented'+x+'_val.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90e30a41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "akiec  (1000, 70, 43)\n",
      "bcc    (1000, 70, 93)\n",
      "bkl    (1011, 80, 217)\n",
      "mel    (1067, 70, 171)\n",
      "nv     (1100, 80, 908)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((5178, 8), (370, 8), (1432, 6))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for label in labels:\n",
    "    list1 = len(train_df[train_df['dx'] == label]), len(val_df[val_df['dx'] == label]), len(test_df[test_df['dx'] == label])\n",
    "    space = ' '\n",
    "    print(label,(5-len(label))*space ,list1)\n",
    "    \n",
    "train_df.shape, val_df.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05200234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5178 validated image filenames belonging to 5 classes.\n",
      "Found 370 validated image filenames belonging to 5 classes.\n",
      "Found 1432 validated image filenames belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "rescale=1./255\n",
    "color_mode = 'rgb'\n",
    "target_size = (32, 32)\n",
    "batch_size  = 64\n",
    "# 600 x 450\n",
    "train_data_np = np.array([img_to_array(load_img(train_path+'/'+img, target_size=target_size)) for img in train_df['image_id'].values.tolist()])\n",
    "\n",
    "datagen = ImageDataGenerator(rescale=rescale,\n",
    "                            featurewise_center=True,\n",
    "                            featurewise_std_normalization=True)\n",
    "datagen.fit(train_data_np)\n",
    "train_set = datagen.flow_from_dataframe(train_df,\n",
    "                                        directory=train_path,\n",
    "                                        x_col=\"image_id\",\n",
    "                                        y_col=\"dx\",\n",
    "                                        color_mode=color_mode,\n",
    "                                        target_size=target_size,\n",
    "                                        batch_size=batch_size,\n",
    "                                        class_mode='categorical',\n",
    "                                        shuffle=False\n",
    "                                        )\n",
    "\n",
    "val_set = datagen.flow_from_dataframe(val_df,\n",
    "                                      directory=train_path,\n",
    "                                      x_col=\"image_id\",\n",
    "                                      y_col=\"dx\",\n",
    "                                      color_mode=color_mode,\n",
    "                                      target_size=target_size,\n",
    "                                      batch_size=batch_size,\n",
    "                                      class_mode='categorical',\n",
    "                                      shuffle=False\n",
    "                                      )\n",
    "test_set = datagen.flow_from_dataframe(test_df,\n",
    "                                       directory=test_path,\n",
    "                                       x_col=\"image_id\",\n",
    "                                       y_col=\"dx\",\n",
    "                                       color_mode=color_mode,\n",
    "                                       target_size=target_size,\n",
    "                                       batch_size=batch_size,\n",
    "                                       class_mode='categorical',\n",
    "                                       shuffle=False\n",
    "                                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa00a34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_plot(model, history, now, save=True):\n",
    "    # convert the history.history dict to a pandas DataFrame:\n",
    "    if type(history) is not pd.DataFrame:\n",
    "        history = pd.DataFrame(history)\n",
    "    if save == True:\n",
    "        hist_csv_file = f'model-comparison/{now}/history.csv'\n",
    "        with open(hist_csv_file, mode='w') as f:\n",
    "            history.to_csv(f) \n",
    "    epochs = range(1, history.shape[0]+1)\n",
    "    plt.figure(figsize=(5, 2))\n",
    "    plt.plot(epochs, history['accuracy'], label='Accuracy')\n",
    "    plt.plot(epochs, history['val_accuracy'], label='Validation Accuracy')\n",
    "    max_val_acc_epoch = np.argmax(history['val_accuracy']) + 1\n",
    "    max_val_acc = history['val_accuracy'][max_val_acc_epoch-1]\n",
    "    label='Best Epoch = '+str(max_val_acc_epoch)+'\\nVal. Acc. = '+str((max_val_acc*100).round(2))+ '%'\n",
    "    plt.plot(max_val_acc_epoch, max_val_acc, 'ro', label=label)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlim([0, history.shape[0]+0.1])\n",
    "    plt.ylim([0.5, 1])\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.legend(loc='upper left')\n",
    "    if save == True:\n",
    "        plt.savefig(f'model-comparison/{now}/val-acc.png')\n",
    "        np.savetxt('model-comparison/{}/{}.txt'.format(now,str((max_val_acc*100).round(2))), [max_val_acc], fmt='%f')\n",
    "        stats = str(now) + ' ' + str((max_val_acc*100).round(2)) + '\\n'\n",
    "        with open('model-comparison/best-models.txt', 'a') as f:\n",
    "            f.write(stats)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07883172",
   "metadata": {},
   "outputs": [],
   "source": [
    "def EvaluateModel(model, test_set, str1, now, save = True):\n",
    "    \n",
    "    print('\\n PREDICTING LABELS OF TEST IMAGES')\n",
    "    result = model.predict(test_set)\n",
    "    y_pred = np.argmax(result, axis=1)\n",
    "    \n",
    "    if save==True:\n",
    "        #save y_pred to csv file\n",
    "        os.mkdir('model-comparison/'+now+'/'+str1)\n",
    "        np.savetxt('model-comparison/{}/{}/pred.csv'.format(now,str1), y_pred, delimiter=',', fmt='%d')\n",
    "    \n",
    "    y_true = test_set.classes # List containing true labels for each image.\n",
    "\n",
    "    # Understanding classification power of model on each class    \n",
    "    report = classification_report(y_true, y_pred, target_names=test_set.class_indices.keys())\n",
    "    report_d = pd.DataFrame(classification_report(y_true, y_pred, output_dict=True, target_names=test_set.class_indices.keys())).transpose()\n",
    "    report_d['support']['accuracy'] = report_d['support']['macro avg']\n",
    "\n",
    "    annot = report_d.copy()\n",
    "    annot.iloc[:, 0:3] = (annot.iloc[:, 0:3]*100).applymap('{:.2f}'.format) + ' %'\n",
    "    annot.iloc[7, 1] = ''\n",
    "    annot.iloc[7, 0] = ''\n",
    "    annot['support'] = annot['support'].astype(int)\n",
    "\n",
    "    # how to save report as image\n",
    "    norm = Normalize(-1,1)\n",
    "    cmap = LinearSegmentedColormap.from_list(\"\", [[norm(-1.0), \"white\"],[norm( 1.0), \"white\"]])\n",
    "    plot = sns.heatmap(report_d, annot=annot, cmap=cmap, cbar=False, fmt='')\n",
    "    fig = plot.get_figure()\n",
    "    if save==True:\n",
    "        fig.savefig('model-comparison/{}/{}/report.png'.format(now,str1))\n",
    "    \n",
    "    f1_score = ((report_d['f1-score']['weighted avg']*100000//10)/100)\n",
    "    accuracy = ((report_d['f1-score']['accuracy']*100000//10)/100)\n",
    "    print('\\nAccuracy of model prediction is: {:.2f} %'.format(accuracy))\n",
    "    print('\\nF1-score of model prediction is: {:.2f} %'.format(f1_score))\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, \n",
    "                              display_labels=test_set.class_indices.keys()\n",
    "                              )\n",
    "    disp.plot(cmap='Reds')\n",
    "    disp.ax_.set_title('Confusion Matrix')\n",
    "    plt.show()\n",
    "    if save==True:\n",
    "        disp.figure_.savefig('model-comparison/{}/{}/cm.png'.format(now,str1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fdccdfb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_new_model(model):\n",
    "    # Extra\n",
    "    #class_weights = class_weight.compute_class_weight(class_weight='balanced',classes=np.unique(train_set.classes), y=train_set.classes)\n",
    "    #class_weights_dict=dict(zip(np.unique(train_set.classes),class_weights))\n",
    "    #keras.utils.set_random_seed(42)     \n",
    "    # inside model.fit: class_weight=class_weights_dict,\n",
    "\n",
    "    # Train new model and evaluate\n",
    "    now = datetime.datetime.now().strftime(\"%d-%m-%H-%M\")\n",
    "    os.mkdir('model-comparison/'+now)\n",
    "    def myprint(s):\n",
    "        with open(f'model-comparison/{now}/modelsummary.txt','a') as f:\n",
    "            print(s, file=f)\n",
    "    model.summary(print_fn=myprint)\n",
    "    with open('model-comparison/last.txt', 'w') as f:\n",
    "        f.write(str(now))\n",
    "    return now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transfer Learning\n",
    "\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "base_model = applications.VGG19(weights='imagenet', include_top=False, input_shape=(target_size[0],target_size[1],3))\n",
    "                                   \n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = True\n",
    "    \n",
    "# Add layers at the end\n",
    "model = base_model.output\n",
    "model = Flatten()(model)\n",
    "\n",
    "model = Dense(512, kernel_initializer='he_uniform')(model)\n",
    "model = Dropout(0.2)(model)\n",
    "model = BatchNormalization()(model)\n",
    "model = Activation('relu')(model)\n",
    "\n",
    "model = Dense(128, kernel_initializer='he_uniform')(model)\n",
    "model = Dropout(0.2)(model)\n",
    "model = BatchNormalization()(model)\n",
    "model = Activation('relu')(model)\n",
    "\n",
    "model = Dense(32, kernel_initializer='he_uniform')(model)\n",
    "model = Dropout(0.2)(model)\n",
    "model = BatchNormalization()(model)\n",
    "model = Activation('relu')(model)\n",
    "\n",
    "output = Dense(len(labels), activation='softmax')(model)\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=output)\n",
    "\n",
    "optimizer = Adam(learning_rate=0.0001)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizer, \n",
    "              metrics=['accuracy'])\n",
    "n_epoch = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-21 20:20:32.381666: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81/81 [==============================] - 88s 1s/step - loss: 1.4383 - accuracy: 0.4023 - val_loss: 2.6935 - val_accuracy: 0.4811 - lr: 1.0000e-04\n",
      "Epoch 2/30\n",
      "81/81 [==============================] - 92s 1s/step - loss: 1.0527 - accuracy: 0.6192 - val_loss: 1.0826 - val_accuracy: 0.6000 - lr: 1.0000e-04\n",
      "Epoch 3/30\n",
      "48/81 [================>.............] - ETA: 35s - loss: 0.9298 - accuracy: 0.6914"
     ]
    }
   ],
   "source": [
    "now = train_new_model(model)\n",
    "\n",
    "# Callbacks\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=10, verbose=1, mode='auto', restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, verbose=1, mode='auto')\n",
    "checkpoint = ModelCheckpoint(f\"model-comparison/{now}/model.h5\", monitor='val_accuracy', save_best_only=True, mode='max')\n",
    "\n",
    "history = model.fit(train_set,\n",
    "                    epochs=30,\n",
    "                    callbacks=[reduce_lr,early_stop,checkpoint],\n",
    "                    validation_data=val_set,\n",
    "                    shuffle=True)\n",
    "\n",
    "\n",
    "# Evaluate Transfer Learning model\n",
    "loss_plot(model, history.history, now)\n",
    "EvaluateModel(model, val_set, 'val', now)\n",
    "EvaluateModel(model, test_set, 'test', now)\n",
    "os.system(finish_sound)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
